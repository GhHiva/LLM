# forward step:
y_pred = [MLP(x_t) for x_t in x_true]
loss = sum([(y_p-y_t)**2 for y_p, y_t in zip(y_pred, y_true)],0.0)

# backward step:
loss.backward() # make the loss ready for gradient part

# updating step:
 for p in MLP(x_true).para(): # para() is the MLP method 
    p.data += 0.01 * p.grad # 0.01 int he learning coefficient


For reapting k times:

for k in range(20):

  y_pred = [MLP(x_t) for x_t in x_true]
  loss = sum([(y_p-y_t)**2 for y_p, y_t in zip(y_pred, y_true)],0.0)

  for p in MLP(x_true).para():
      p.grad = 0.0
  loss.backward()

  for p in MLP(x_true).para():
      p.data += 0.01 * p.grad

  print(k, loss.data)
